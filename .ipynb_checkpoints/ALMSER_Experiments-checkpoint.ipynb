{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a single ALMSER experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/magellan_restaurants/feature_vector_files/\"\n",
    "output_path = \"datasets/magellan_restaurants/almser/\"\n",
    "fv_splitter = \"__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active Learning Settings\n",
    "max_queries =25\n",
    "runs = 1\n",
    "query_strategy = 'disagreement' #disagreement, random, disagreeement_stratified, disagreement_post_graph\n",
    "files = os.listdir(path)\n",
    "files =[f.replace('.csv','') for f in files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Passive Learning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from learningutils import *\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from collections import Counter \n",
    "\n",
    "pairs_fv_train= pd.read_csv(output_path+\"train_pairs_fv.csv\")\n",
    "pairs_fv_test= pd.read_csv(output_path+\"test_pairs_fv.csv\")\n",
    "\n",
    "metadata_columns = ['source_id','target_id','pair_id', 'agg_score','source','target', 'label']\n",
    "train_X = pairs_fv_train.drop(metadata_columns, axis=1)\n",
    "train_y = pairs_fv_train['label']\n",
    "\n",
    "test_X = pairs_fv_test.drop(metadata_columns, axis=1)\n",
    "test_y = pairs_fv_test['label']\n",
    "\n",
    "\n",
    "model = getClassifier('rf')\n",
    "model.fit(train_X,train_y)\n",
    "predictions = model.predict(test_X)\n",
    "prec, recall, fscore, support  = precision_recall_fscore_support(test_y, predictions, average='binary')\n",
    "\n",
    "print(\"Passive learing results: %f P, %f R, %f F1\" % (prec,recall,fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the stored files and start ALMSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scoreaggregation import *\n",
    "from ALMSER import *\n",
    "from ALMSER_EXP import *\n",
    "\n",
    "\n",
    "almser_path = output_path\n",
    "print(almser_path)\n",
    "\n",
    "pairs_fv_train= pd.read_csv(almser_path+\"train_pairs_fv.csv\")\n",
    "\n",
    "pairs_fv_train['datasource_pair'] = pairs_fv_train['source'].str.rsplit('_', 1).str[0]+fv_splitter+pairs_fv_train['target'].str.rsplit('_', 1).str[0]\n",
    "\n",
    "pairs_fv_test= pd.read_csv(almser_path+\"test_pairs_fv.csv\")\n",
    "pairs_fv_test['datasource_pair'] = pairs_fv_test['source'].str.rsplit('_', 1).str[0]+fv_splitter+pairs_fv_test['target'].str.rsplit('_', 1).str[0]\n",
    "\n",
    "all_nodes_test_match = set(pairs_fv_test[pairs_fv_test.label]['source'].values)\n",
    "all_nodes_test_match.update(set(pairs_fv_test[pairs_fv_test.label]['target'].values))\n",
    "\n",
    "all_nodes_train_match = set(pairs_fv_train[pairs_fv_train.label]['source'].values)\n",
    "all_nodes_train_match.update(set(pairs_fv_train[pairs_fv_train.label]['target'].values))\n",
    "\n",
    "print(\"Intersection:\", all_nodes_train_match.intersection(all_nodes_test_match))\n",
    "\n",
    "unique_source_pairs = files\n",
    "results_concat = pd.DataFrame(columns=['P_model','R_model','F1_model_micro','F1_model_macro','F1_model_micro_boot','F1_model_macro_boot'])\n",
    "results_all = pd.DataFrame(columns=['P','P_std','R','R_std','F1_micro','F1_micro_std','F1_macro','F1_macro_std','F1_micro_boot',\n",
    "                                   'F1_micro_boot_std','F1_macro_boot', 'F1_macro_boot_std' ])\n",
    "\n",
    "for run in range(runs):\n",
    "    print(\"RUN %i\" % run)\n",
    "    \n",
    "    almser_exp = ALMSER_EXP(pairs_fv_train, pairs_fv_test, unique_source_pairs, max_queries, 'rf',\n",
    "                        query_strategy, fv_splitter, bootstrap=True)\n",
    "    almser_exp.run_AL()\n",
    "    \n",
    "    results_concat= pd.concat((results_concat,(almser_exp.results[['P_model','R_model','F1_model_micro','F1_model_macro','F1_model_micro_boot','F1_model_macro_boot','F1_model_micro_boost_graph']])))\n",
    "\n",
    "results_concat_by_row_index = results_concat.groupby(results_concat.index)\n",
    "results_concat_mean =results_concat_by_row_index.mean(numeric_only=False) \n",
    "results_concat_std =results_concat_by_row_index.apply(np.std)\n",
    "\n",
    "\n",
    "results_all['P'] = results_concat_mean['P_model']\n",
    "results_all['P_std'] = results_concat_std['P_model']\n",
    "results_all['R'] = results_concat_mean['R_model']\n",
    "results_all['R_std'] = results_concat_std['R_model']\n",
    "results_all['F1_micro'] = results_concat_mean['F1_model_micro']\n",
    "results_all['F1_micro_std'] = results_concat_std['F1_model_micro']\n",
    "results_all['F1_macro'] = results_concat_mean['F1_model_macro']\n",
    "results_all['F1_macro_std'] = results_concat_std['F1_model_macro']\n",
    "results_all['F1_micro_boot'] = results_concat_mean['F1_model_micro_boot']\n",
    "results_all['F1_micro_boot_std'] = results_concat_std['F1_model_micro_boot']\n",
    "results_all['F1_macro_boot'] = results_concat_mean['F1_model_macro_boot']\n",
    "results_all['F1_macro_boot_std'] = results_concat_std['F1_model_macro_boot']\n",
    "results_all['F1_model_micro_boost_graph'] = results_concat_mean['F1_model_micro_boost_graph']\n",
    "results_all['F1_model_micro_boost_graph_std'] = results_concat_std['F1_model_micro_boost_graph']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from scoreaggregation import *\n",
    "from ALMSER import *\n",
    "from ALMSER_EXP import *\n",
    "from networkx.algorithms.flow import *\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "G.add_edge(\"a\", \"c\", capacity=0.2)\n",
    "G.add_edge(\"c\", \"d\", capacity=0.1)\n",
    "G.add_edge(\"c\", \"e\", capacity=0.7)\n",
    "G.add_edge(\"e\", \"f\", capacity=0.9)\n",
    "G.add_edge(\"a\", \"d\", capacity=0.3)\n",
    "G.add_edge(\"f\", \"b\", capacity=3.0)\n",
    "\n",
    "\n",
    "drawGraph(G)\n",
    "\n",
    "cut_value, partition = nx.minimum_cut(G, 'd','b')\n",
    "reachable, non_reachable = partition\n",
    "\n",
    "print(partition)\n",
    "print(reachable)\n",
    "print(non_reachable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2_13222\n",
    "from networkx.algorithms import community\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "cc_of_node = nx.node_connected_component(almser_exp.G, '2_13222')\n",
    "print(\"Connected component size:  \",len(cc_of_node))\n",
    "\n",
    "subg = almser_exp.G.subgraph(cc_of_node)\n",
    "pos = nx.spring_layout(subg)\n",
    "plt.figure(3,figsize=(15,12))   \n",
    "nx.draw(subg,pos,edge_color='black',node_size=500,node_color='pink',alpha=0.9,linewidths=1,\n",
    "        labels={node:node for node in subg.nodes()})\n",
    "edge_labels = nx.get_edge_attributes(subg,'capacity')\n",
    "nx.draw_networkx_edge_labels(subg,pos,edge_labels=edge_labels,font_color='red')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.flow import *\n",
    "check = almser_exp.labeled_set[almser_exp.labeled_set.label==False]\n",
    "cut_labels = []\n",
    "cuts_all= []\n",
    "for index,row in check.iterrows():\n",
    "    if (almser_exp.G.has_node(row.source) and almser_exp.G.has_node(row.target)): \n",
    "        if (nx.has_path(almser_exp.G, row.source,row.target)):\n",
    "            print(\"Found path between : %s - %s \" %(row.source,row.target))\n",
    "            cc_of_node = nx.node_connected_component(almser_exp.G, row.source)\n",
    "            print(\"Connected component size:  \",len(cc_of_node))\n",
    "            \n",
    "            subg = almser_exp.G.subgraph(cc_of_node)\n",
    "            drawGraph(subg)\n",
    "            \n",
    "            cut_weight, partitions = nx.minimum_cut(subg, row.source, row.target)\n",
    "            \n",
    "            edge_cut_list = [] # Computed by listing edges between the 2 partitions\n",
    "            for p1_node in partitions[0]:\n",
    "                for p2_node in partitions[1]:\n",
    "                    if subg.has_edge(p1_node,p2_node):\n",
    "                        edge_cut_list.append((p1_node,p2_node))\n",
    "                        cut_label = almser_exp.unlabeled_set_metadata[((almser_exp.unlabeled_set_metadata.source==p1_node) & (almser_exp.unlabeled_set_metadata.target==p2_node)) |  ((almser_exp.unlabeled_set_metadata.source==p2_node) & (almser_exp.unlabeled_set_metadata.target==p1_node))].label\n",
    "                        \n",
    "                        print(\"Cut label\", cut_label)\n",
    "            \n",
    "almser_exp.G.remove_edges_from(edge_cut_list)\n",
    "print(edge_cut_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'usapartsdirect_harddrivesdirect'\n",
    "record_pairs_train = almser_exp.labeled_set[almser_exp.labeled_set.datasource_pair == task]\n",
    "\n",
    "train_X, train_y = almser_exp.get_feature_vector_subset(record_pairs_train)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y, test_size=0.33, random_state=42)\n",
    "model = getClassifier(almser_exp.classifier_name, n_estimators=10, random_state=1)\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test).tolist()\n",
    "accur = accuracy_score(y_test, pred)\n",
    "print(accur)\n",
    "\n",
    "print(task)\n",
    "total_dis = almser_exp.unlabeled_set.disagreement.values.sum()\n",
    "print(total_dis)\n",
    "almser_exp.unlabeled_set[almser_exp.unlabeled_set.datasource_pair==task].disagreement.values.sum()/total_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_ = almser_exp.labeled_set.tail(100)\n",
    "\n",
    "tail_[tail_.predicted_label==tail_.graph_inferred_label].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "almser_exp.unlabeled_set.sort_values('inf_score', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "almser_exp.unlabeled_set['label'] = almser_exp.unlabeled_set_metadata.label\n",
    "check = almser_exp.unlabeled_set[almser_exp.unlabeled_set.predicted_label!=almser_exp.unlabeled_set.label]\n",
    "check = check[['votes','disagreement', 'graph_inferred_label','clusters_inferred_label','predicted_label','label']]\n",
    "display(check.head(10))\n",
    "low_disagr = check[check.disagreement>-1.0]\n",
    "print(\"Low disagreement:\", low_disagr.shape[0])\n",
    "print(\"Correct only graph:\", low_disagr[low_disagr.graph_inferred_label==low_disagr.label].shape[0])\n",
    "print(\"Correct only clusters:\", low_disagr[low_disagr.clusters_inferred_label==low_disagr.label].shape[0])\n",
    "print(\"Correct only predicted:\", low_disagr[low_disagr.predicted_label==low_disagr.label].shape[0])\n",
    "print(\"Correct only dt:\", low_disagr[low_disagr.label==[votes[1] for votes in low_disagr.votes]].shape[0])\n",
    "print(\"Correct only gboost:\", low_disagr[low_disagr.label==[votes[2] for votes in low_disagr.votes]].shape[0])\n",
    "print(\"Correct only logr:\", low_disagr[low_disagr.label==[votes[3] for votes in low_disagr.votes]].shape[0])\n",
    "print(\"Correct only svm:\", low_disagr[low_disagr.label==[votes[4] for votes in low_disagr.votes]].shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write results\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp= now.strftime(\"%d_%m_%H_%M\")\n",
    "filename = \"%i_runs_%i_iter_%s_%s\" %(runs,max_queries,query_strategy,timestamp)\n",
    "\n",
    "almser_exp.results.to_csv(output_path+filename+\"_ALL.csv\", index=False)\n",
    "almser_exp.labeled_set.to_csv(output_path+filename+\"_LABELED_SET_INFO.csv\", index=False)\n",
    "almser_exp.informants_eval.to_csv(output_path+filename+\"_INFORMANTS_EVAL.csv\", index=False)\n",
    "results_all.to_csv(output_path+filename+\".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns;\n",
    "almser_exp.get_heatmap_of_iteration()[0]\n",
    "\n",
    "heatm = almser_exp.get_heatmap_of_iteration()[0].fillna(value=1)\n",
    "ax = sns.heatmap(heatm, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, recall, fscore, support  = precision_recall_fscore_support(almser_exp.unlabeled_set_metadata.label, almser_exp.unlabeled_set.clusters_inferred_label, average='binary')\n",
    "print(\"Clusters Precision \", prec)\n",
    "print(\"Clusters Recall \", recall)\n",
    "print(\"Clusters Fscore \", fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import community\n",
    "communities_generator = community.girvan_newman(almser_exp.G)\n",
    "top_level_communities = next(communities_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_com = sorted(map(sorted, top_level_communities))\n",
    "pos_pairs = []\n",
    "for com in top_com:\n",
    "    for i in range(len(com)):\n",
    "        for j in range(i+1,len(com)):\n",
    "            pos_pairs.append((com[i],com[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data = almser_exp.get_feature_vector_subset(almser_exp.unlabeled_set, getLabels=False)\n",
    "\n",
    "predicted_labels = almser_exp.learning_models['all'].predict(unlabeled_data)\n",
    "almser_exp.unlabeled_set['clusters_inferred_label'] = False\n",
    "for ind, row in almser_exp.unlabeled_set[predicted_labels].iterrows():\n",
    "    if (row.source,row.target) in pos_pairs or (row.target,row.source) in pos_pairs:\n",
    "        almser_exp.unlabeled_set.at[ind, 'clusters_inferred_label']=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(almser_exp.labeled_set.cc_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis if task frequency, f1, transferrability on the 150 iteration\n",
    "further_stats= pd.DataFrame(columns=['Task','Annotations','Task-F1'])\n",
    "\n",
    "freq_counter = dict(Counter(almser_exp.labeled_set.datasource_pair))\n",
    "for i in range(len(almser_exp.unique_source_pairs)):\n",
    "    task = almser_exp.unique_source_pairs[i]\n",
    "    f1_score_task = almser_exp.results.tail(1).F1_pairwise_model.values[0].get(task)\n",
    "    freq = freq_counter.get(task)\n",
    "    space_coverage=almser_exp.unlabeled_set[almser_exp.unlabeled_set.datasource_pair==task].head(1).dataspace_coverage.values[0]\n",
    "    explore_score = almser_exp.unlabeled_set[almser_exp.unlabeled_set.datasource_pair==task].head(1).explore_score.values[0]\n",
    "    exploit = almser_exp.unlabeled_set[almser_exp.unlabeled_set.datasource_pair==task].head(1).exploit_score.values[0]\n",
    "\n",
    "    further_stats = further_stats.append({'Task': task, 'Annotations': freq,'Task-F1': f1_score_task, 'Explore_Score': explore_score, 'Exploit_Score':exploit}, ignore_index=True)\n",
    "\n",
    "further_stats\n",
    "#further_stats.to_csv(output_path+\"FURTHER_STATS_\"+filename+\".csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
