{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile each matching task of the setting along the five dimensions:\n",
    "1. Schema Complexity : number of relevant attributes\n",
    "2. Sparsity : ratio of missing values to all attribute values of all relevant attributes\n",
    "3. Size : size of training and validation set\n",
    "4. Corner Cases : apply the corner cases with optimal threshold heuristic and calculate (#false_positives + #false_negatives)/ matching_pairs\n",
    "5. Textuality : average length value in words of top relevant attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_matching_task_profile_info()\n",
    "\n",
    "importantProfilingDimensions()\n",
    "#display(matching_tasks_profiling)\n",
    "#matching_tasks_summary\n",
    "#get_heatmap_of_setting()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datautils import*\n",
    "import os\n",
    "import os.path as path\n",
    "from learningutils import *\n",
    "from sklearn import tree\n",
    "from matching_task import *\n",
    "import time\n",
    "import glob\n",
    "\n",
    "\n",
    "matching_tasks_summary = pd.DataFrame(columns=['Dataset', '#records_source', '#records_target', 'count_record_pairs', '#match', '#non-match',\n",
    "                                               'count_attr','#short_string_attr', '#long_string_attr', '#numeric_attr','avg_density_all'])\n",
    "matching_tasks_baseline_rf_results = pd.DataFrame(columns=['Dataset', 'precision','recall','f1','f1_std','proba_scores',\n",
    "                                                          'proba_scores_std','x-val f1','x-val f1 sigma'])\n",
    "matching_tasks_baseline_svm_results = pd.DataFrame(columns=['Dataset', 'precision','recall','f1','f1_std','proba_scores',\n",
    "                                                      'proba_scores_std','x-val f1','x-val f1 sigma'])\n",
    "\n",
    "matching_tasks_profiling = pd.DataFrame(columns=['Dataset','F1_xval_max', 'F1_xval_top_matching_relevant_features', \n",
    "                                                 'matching_relevant_features', \n",
    "   'matching_relevant_attributes','matching_relevant_attributes_density','matching_relevant_attributes_count',\n",
    " 'matching_relevant_attributes_datatypes','top_matching_relevant_features','top_relevant_attributes', \n",
    " 'top_relevant_attributes_count','top_relevant_attributes_datatypes', 'top_relevant_attributes_density',\n",
    "'avg_length_tokens_top_relevant_attributes','avg_length_words_top_relevant_attributes','corner_cases_top_matching_relevant_features'])\n",
    " \n",
    "# Use the flags below to indicate which results should be calculates\n",
    "summaryFeatures=True\n",
    "baselineResults=False\n",
    "profilingFeatures = True                                                           \n",
    "\n",
    "\n",
    "main_path = '../datasets/musicbrainz20K_smaller/'\n",
    "source_folder=\"sources\"\n",
    "fv_folder = \"feature_vector_files/\"\n",
    "\n",
    "#add the correct separators of the source sets and the gold standard\n",
    "sep_for_source_files= ','\n",
    "gs_sep = ','\n",
    "train_test_val=False # otherwise nested x-validation for baseline experiments\n",
    "fv_name_split = \"_\"\n",
    "#change for allowing multithreading\n",
    "threads=-1\n",
    "\n",
    "def get_matching_task_profile_info():\n",
    "     \n",
    "    dat_counter = 0\n",
    "    for f in glob.glob(main_path+fv_folder+\"/*\"):\n",
    "        dataset_name = f.split(\"/\")[-1].replace(\".csv\",\"\")\n",
    "        print(\"Current dataset pair %s\" %dataset_name)\n",
    "        ds1_name = dataset_name.split(fv_name_split)[0]\n",
    "        ds2_name = dataset_name.split(fv_name_split)[1]\n",
    "        \n",
    "        feature_vector = pd.read_csv(f)\n",
    "       \n",
    "\n",
    "        gs = feature_vector[['source_id','target_id','label']].copy()\n",
    "        gs.rename(columns={'label':'matching'}, inplace=True)\n",
    "\n",
    "        ds1= pd.read_csv(main_path+source_folder+\"/\"+ds1_name+\".csv\", sep =sep_for_source_files, engine='python')\n",
    "        ds2= pd.read_csv(main_path+source_folder+\"/\"+ds2_name+\".csv\", sep =sep_for_source_files, engine='python')\n",
    "        \n",
    "        #ds1.drop(columns=['cluster_id'], inplace=True)\n",
    "\n",
    "        ds1.rename(columns={'id':'subject_id'}, inplace=True)\n",
    "        #ds2.drop(columns=['cluster_id'], inplace=True)\n",
    "        ds2.rename(columns={'id':'subject_id'}, inplace=True)\n",
    "\n",
    "        if not ds1.empty and not ds2.empty and not gs.empty:\n",
    "            ds1['subject_id'] = ds1['subject_id'].apply(str)\n",
    "            ds2['subject_id'] = ds2['subject_id'].apply(str)\n",
    "\n",
    "\n",
    "            gs['source_id'] = gs['source_id'].apply(str)\n",
    "            gs['target_id'] = gs['target_id'].apply(str)\n",
    "\n",
    "            common_attributes = [value for value in ds1.columns if (value in ds2.columns and value!='subject_id')] \n",
    "            matching_task = MatchingTask(ds1, ds2, gs, feature_vector, common_attributes)\n",
    "\n",
    "            if (summaryFeatures):\n",
    "                matching_task.getSummaryFeatures()\n",
    "                #correspondes features\n",
    "                summary_features = matching_task.dict_summary\n",
    "                summary_features['Dataset'] = dataset_name\n",
    "\n",
    "                for key in matching_tasks_summary.columns:\n",
    "                    matching_tasks_summary.loc[dat_counter, key] = summary_features.get(key)\n",
    "\n",
    "\n",
    "            if (baselineResults):\n",
    "                # get baseline results\n",
    "                if (train_test_val): \n",
    "                    print(\"Evaluation with train_validation_test split\")\n",
    "                    matching_task.getSplitValidationResults(model=\"linear\")\n",
    "                    matching_task.getSplitValidationResults(model=\"non-linear\")\n",
    "                else:\n",
    "                    print(\"Evaluation with Nested-X-Validation (no splits will be considered) - slow for large tasks\")\n",
    "                    matching_task.getNestedXValidationResults(model=\"linear\")\n",
    "                    matching_task.getNestedXValidationResults(model=\"non-linear\")\n",
    "\n",
    "                #linear model results\n",
    "                linear_results = matching_task.dict_linear_results\n",
    "                linear_results['Dataset'] = dataset_name\n",
    "                for key in linear_results:\n",
    "                    matching_tasks_baseline_svm_results.loc[dat_counter, key] = linear_results.get(key)\n",
    "                #non-linear model results\n",
    "                non_linear_results = matching_task.dict_non_linear_results\n",
    "                non_linear_results['Dataset'] = dataset_name\n",
    "                for key in non_linear_results:\n",
    "                    matching_tasks_baseline_rf_results.loc[dat_counter, key] = non_linear_results.get(key)\n",
    "\n",
    "            if(profilingFeatures):\n",
    "                matching_task.getProfilingFeatures()\n",
    "                ident_features_profile =  matching_task.dict_profiling_features\n",
    "                ident_features_profile['Dataset'] = dataset_name\n",
    "                for key in matching_tasks_profiling.columns:\n",
    "                    matching_tasks_profiling.loc[dat_counter,key] = ident_features_profile.get(key)                                  \n",
    "\n",
    "\n",
    "            dat_counter+=1\n",
    "\n",
    "    \n",
    "def displaySaveResults(): \n",
    "    timestr = time.strftime(\"%m%d_%H%M%S\")\n",
    "    if (profilingFeatures):\n",
    "        display(matching_tasks_summary)\n",
    "        matching_tasks_summary.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_profiling.csv', index=False)\n",
    "\n",
    "    if (baselineResults):\n",
    "        display(matching_tasks_svm_results)\n",
    "        display(matching_tasks_rf_results)\n",
    "        matching_tasks_rf_results.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_RF_results.csv', index=False)\n",
    "        matching_tasks_svm_results.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_SVM_results.csv', index=False)\n",
    "\n",
    "    if(profilingFeatures):\n",
    "        display(matching_tasks_profiling)\n",
    "        matching_tasks_profiling.to_csv(main_path+source_folder+'/'+timestr+'matching_tasks_profiling_features_summary.csv', index=False)\n",
    "\n",
    "      \n",
    "def importantProfilingDimensions():\n",
    "    profiling_dimensions = pd.DataFrame(columns=['Dataset'])\n",
    "    profiling_dimensions['Dataset'] = matching_tasks_summary.Dataset\n",
    "    profiling_dimensions['Size'] = matching_tasks_summary.count_record_pairs\n",
    "    profiling_dimensions['Match#'] = matching_tasks_summary['#match']\n",
    "    profiling_dimensions = pd.merge(profiling_dimensions, matching_tasks_profiling)\n",
    "    \n",
    "    for index, row in profiling_dimensions.iterrows():\n",
    "        relev_attr = row['matching_relevant_attributes']\n",
    "        \n",
    "        top_relev_attr = row['top_relevant_attributes']\n",
    "        \n",
    "        format_relev_attr = []\n",
    "        for ra in relev_attr:\n",
    "            if ra in top_relev_attr: format_relev_attr.append(ra+\"*\")\n",
    "            else: format_relev_attr.append(ra)\n",
    "        profiling_dimensions.loc[index,'matching_relevant_attributes']=format_relev_attr\n",
    "        \n",
    "    columns = ['Dataset pair', 'F1-xval_all_attr', 'Relevant Attributes', 'Top Features', 'Schema Complexity', 'Textuality', 'Sparsity', 'Size', 'Match#', 'Corner Cases']\n",
    "    profiling_dimensions.rename(columns={'Dataset':columns[0], 'F1_xval_max':columns[1],'matching_relevant_attributes':columns[2], 'top_matching_relevant_features':columns[3],\n",
    "                                       'matching_relevant_attributes_count':columns[4], 'avg_length_words_top_relevant_attributes':columns[5],\n",
    "                                        'matching_relevant_attributes_density':columns[6], 'corner_cases_top_matching_relevant_features':columns[9]}, inplace=True)\n",
    "    \n",
    "    profiling_dimensions= profiling_dimensions[columns]\n",
    "    profiling_dimensions['Sparsity'] = 1-profiling_dimensions['Sparsity']\n",
    "    profiling_dimensions.sort_values(by=['Dataset pair'], inplace=True)  \n",
    "    display(profiling_dimensions)\n",
    "    profiling_dimensions.to_csv(main_path+\"profiling.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
